{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1. Heart Disease Classification\n",
    "\n",
    "#### Joonas Lehikoinen, Przemyslaw Zuchmanski\n",
    "##### 31.01.2020\n",
    "### Helsinki Metropolia University of Applied Sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main object is to created and train a dense neural network to predict the presence of heart disease on the base of heart disease cleveland data downloaded from the site: https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data\n",
    "The data contains values of various health factors usefull in detecting heart diseases. There are 13 factors described in 13 coluns. 14th column describes if the patint sufers from heart disease. The number of records is 303. Missing values (detected in 6 raws) were replaced with 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#names of columns\n",
    "names = [\"age\",\"sex\",\"cp\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\n",
    "                            \"thalach\",\"examg\",\"oldpeak\",\"slope\",\"ca\",\"thal\",\"num\"]\n",
    "                   \n",
    "#reading data and giving names for columns, detecting NaN valuess\n",
    "df = pd.read_csv(\"processed.cleveland.data\", \n",
    "                 names=names,\n",
    "                     header=None, \n",
    "                     index_col = None, \n",
    "                     na_values = '?')\n",
    "\n",
    "#replacing NaN values with 0\n",
    "df = df.replace(numpy.NaN,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic statistics are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape of data set: ', df.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Division data into two subsets: <br>\n",
    "data - all health factors <br>\n",
    "labels - indicate if the person is rather sick (1) or healthy (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing set to data and labels\n",
    "data = df.drop(['num'], axis=1)\n",
    "#converting labels to binary atribut\n",
    "label = 1.0*(df['num'] >0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Models and training\n",
    "Dividing data. For training and validating we use 80% of samples. Remains 20% we will use for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and training\n",
    "Dividing data. For training and validating we use 80% of samples. Remains 20% we will use for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "                    data,\n",
    "                    label,\n",
    "                    test_size = 0.2,\n",
    "                    random_state = 39,\n",
    "                    shuffle = True)\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "test_data -= mean\n",
    "test_data /= std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "in order to makes testing and showing the resuts easier we made functions\n",
    "\n",
    "#### Fit and make model\n",
    "Fitting the model. We use 20% of remaining data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Activation, Flatten, Dropout, Dense, Embedding, TimeDistributed\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "#from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "history = model\n",
    "def fitModel(num_epo,batch,neuron_amount):\n",
    "    \n",
    "    \n",
    "    #creating layers and defining parameters\n",
    "    LSTM_layer_num=3\n",
    "    layer_size = [neuron_amount]\n",
    "    activation_f=\"relu\"\n",
    "    dropout=0.2\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "    model.add(Dense(layer_size[0], input_shape =(13,),activation=activation_f,kernel_regularizer=regularizers.l2(0.05)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(layer_size[0], activation=activation_f,kernel_regularizer=regularizers.l2(0.05)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "    ###Checkpoint###\n",
    "    checkpoint_name = 'Disease3x128Batch16.hdf5'\n",
    "    checkpoint = ModelCheckpoint(checkpoint_name, monitor='loss', verbose = 0, save_best_only = True, mode ='min')\n",
    "    callbacks_list = [checkpoint]  \n",
    "    \n",
    "      \n",
    "    \n",
    "    # Fit the model :\n",
    "    global history\n",
    "    model_params = {'epochs': num_epo,\n",
    "                    'batch_size': batch,\n",
    "                    'callbacks': callbacks_list,\n",
    "                    'verbose': 0,\n",
    "                    'validation_split': 0.20,\n",
    "                    'shuffle': True,\n",
    "                    'initial_epoch': 1,\n",
    "                    'steps_per_epoch': None,\n",
    "                    'validation_steps': None}\n",
    "\n",
    "    \n",
    "    history=model.fit(train_data.values,\n",
    "              train_labels.values,\n",
    "               epochs = model_params['epochs'],\n",
    "               batch_size = model_params['batch_size'],\n",
    "               callbacks= model_params['callbacks'],\n",
    "               verbose = model_params['verbose'],\n",
    "               validation_split = model_params['validation_split'],\n",
    "               shuffle = model_params['shuffle']    \n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ploting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    # Plot the loss score and mean absolute error for both training and validation setss\n",
    "\n",
    "    #coleting data from history\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    mae = history.history['acc']\n",
    "    val_mae = history.history['val_acc']\n",
    "\n",
    "    #defining time axis\n",
    "    time = range(1,len(loss)+1)\n",
    "\n",
    "    #ploting loss vs Epochs\n",
    "    #loss of validation set is red\n",
    "    plt.plot(time, loss, 'b-')\n",
    "    plt.plot(time, val_loss, 'r-')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "    #ploting accuracy vs Epochs\n",
    "    #accuracy of validation set is red\n",
    "    plt.plot(time, mae, 'b-')\n",
    "    plt.plot(time, val_mae, 'r-')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('ACC')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def loadWeights(weightFile):\n",
    "    weights_file = weightFile # weights file path\n",
    "    model.load_weights(weights_file)\n",
    "    model.compile(loss = 'mse', optimizer = 'adam',metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Results\n",
    "Main tried combinations\n",
    "### 1 network setup:\n",
    "model.add(Dense(64, input_shape =(13,),activation='relu',kernel_regularizer=regularizers.l2(0.05)))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.05)))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "Different batch sizes for 400 epochs were checked. In this set up it sims to be that batch size 30 is the best\n",
    "\n",
    "number of epochs: 400, batch size: 10\n",
    "loss: 0.9030 - acc: 0.8361\n",
    "plots of loss and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitModel(250,40,84)\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data.values, test_labels.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
